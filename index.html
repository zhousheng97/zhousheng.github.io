<head>
    <style>
        /* 全局字体设置 */
        body {
            font-size: 14px;  /* 调整整体字体大小 */
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            text-align: center;
        }

        /* 主要内容表格 */
        table {
            width: 90%;  /* 让整个页面更宽 */
            max-width: 1200px; /* 最大宽度 */
            margin: 0 auto; /* 居中显示 */
            border-collapse: collapse;
        }

        td {
            padding: 15px;  /* 让内容更紧凑 */
            text-align: left;
        }

        /* 标题调整 */
        h2 {
            font-size: 18px; /* 让标题稍小 */
            text-align: left;
            padding-left: 20px;
        }

        /* 个人信息部分 */
        .profile-name {
            font-size: 30px; /* 让名字稍小 */
        }

        /* 链接样式 */
        a {
            font-size: 13px;
        }

        /* 论文展示区域 */
        .publication-img {
            width: 100%;
            max-width: 100%;
        }

        /* 访问地图调整 */
        .click-map img {
            width: 250px;  /* 调整地图大小 */
            height: 180px;
        }
    </style>
</head>

<body>
    <table>
        <tr>
            <td>
                <table width="100%">
                    <tr>
                        <td width="67%" valign="middle">
                            <p class="profile-name" align="center">
                                <strong>Sheng Zhou</strong>
                            </p>
                            <p>
                                My research focuses on vision and language understanding and reasoning. 
                                I specialize in fine-grained image and video understanding, with an emphasis on scene text-based QA. 
                                Currently, my research focuses on scene-text VideoQA and multimodal large language models.
                            </p>
                            <p align="center">
                                <a href="mailto:hzgn97@gmail.com">Email</a> &nbsp;/&nbsp;
                                <a href="https://github.com/zhousheng97">Github</a> &nbsp;/&nbsp;
                                <a href="https://scholar.google.com/citations?user=7r9ejvcAAAAJ&hl=zh-CN">Google Scholar</a>
                            </p>
                        </td>
                        <td width="33%" align="center">
                            <img src="./images/zhousheng.jpg" width="160" height="160" alt="Profile Picture">
                        </td>
                    </tr>
                </table>

                <hr />

                <h2>Publications and Preprints</h2>
                <table>
                    <tr>
                        <td width="30%" align="center">
                            <img class="publication-img" src="./images/egotextvqa" alt="EgoTextVQA">
                        </td>
                        <td width="70%">
                            <strong>EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering</strong>
                            <br>
                            **Sheng Zhou**, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat Seng Chua, Angela Yao
                            <br>
                            <div class="item-meta col-12 col-md-12 col-lg-14">
                                [ <a href="https://arxiv.org/abs/2502.07411">CVPR'25</> 
                                / <a href="https://zhousheng97.github.io/EgoTextVQA_page/">Project Page</>
                                / <a href="https://github.com/zhousheng97/EgoTextVQA">Data and Code</a>
                                / <a href="">Cite</a>]
                            </div>      
                        </td>

                        <td width="30%" align="center">
                            <img class="publication-img" src="./images/vitxtgqa" alt="ViTXT-GQA">
                        </td>
                        <td width="70%">
                            <strong>Scene-Text Grounding for Text-Based Video Question Answering</strong>
                            <br>
                            **Sheng Zhou**, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
                            <br>
                            <div class="item-meta col-12 col-md-12 col-lg-14">
                                [ <a href="https://arxiv.org/abs/2409.14319">CVPR'25</> 
                                / <a href="https://github.com/zhousheng97/ViTXT-GQA">Data and Code</a>
                                / <a href="">Cite</a>]
                            </div>   
                        </td>

                        <td width="30%" align="center">
                            <img class="publication-img" src="./images/vitxtgqa" alt="ViTXT-GQA">
                        </td>
                        <td width="70%">
                            <strong>Graph Pooling Inference Network for Text-based VQA</strong>
                            <br>
                            **Sheng Zhou**, Dan Guo, Xun Yang, Jianfeng Dong, Meng Wang
                            <br>
                            <div class="item-meta col-12 col-md-12 col-lg-14">
                                [ <a href="https://dl.acm.org/doi/10.1145/3634918">ACM TOMM</> 
                                / <a href="https://github.com/zhousheng97/GPIN">Code</a>
                                / <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:s8vewsJ3DIEJ:scholar.google.com/&output=citation&scisdr=ClHfKN6iEKHbin0abhE:AFWwaeYAAAAAZ8wcdhHFupsB6K0DC6lC2V8_fV8&scisig=AFWwaeYAAAAAZ8wcduR9824PiOiFMtiITL4_Ssw&scisf=4&ct=citation&cd=-1&hl=zh-CN">Cite</a>]
                            </div>   
                        </td>

                        <td width="30%" align="center">
                            <img class="publication-img" src="./images/vitxtgqa" alt="ViTXT-GQA">
                        </td>
                        <td width="70%">
                            <strong>Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA</strong>
                            <br>
                            **Sheng Zhou**, Dan Guo, Jia Li, Xun Yang, Meng Wang
                            <br>
                            <div class="item-meta col-12 col-md-12 col-lg-14">
                                [ <a href="https://ieeexplore.ieee.org/document/10241306">IEEE TIP</> 
                                / <a href="https://github.com/zhousheng97/SSGN">Code</a>
                                / <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:CYa7VAxZrIsJ:scholar.google.com/&output=citation&scisdr=ClHHg-wHEKHbin0bq6I:AFWwaeYAAAAAZ8wds6KvJYXXchKN5x59CRon7Vs&scisig=AFWwaeYAAAAAZ8wds50FmgspN0j_OTxWbfu1QoM&scisf=4&ct=citation&cd=-1&hl=zh-CN&scfhb=1">Cite</a>]
                            </div>  
                        </td>
                    </tr>
                </table>

                <hr />

                <h2>Academic Services</h2>
                <ul>
                    <li><p> ... </p></li>
                </ul>

                <hr />

                <h2>Honors and Awards</h2>
                <ul>
                    <li><p> [2025.03] <span style="color: #4f69c6"><strong>...</strong></span> </p></li>
                </ul>

                <hr />

                <!-- 点击地图 -->
                <p class="click-map">
                    <a href="https://clustrmaps.com/site/1c4rm" title="ClustrMaps">
                        <img src="//www.clustrmaps.com/map_v2.png?d=Icn5np1Ziy1nrdxuhNF243TFUnolCVjDuXNjTMrnhOg&cl=ffffff" />
                    </a>
                </p>
            </td>
        </tr>
    </table>
</body>
